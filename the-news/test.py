# -*- coding: utf-8 -*-
"""Zach, Cris, Safwan || AIIE Gen AI Project with Gemini API Integration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zib2e1ocdC4dtf9xvmwKqOTso7n8Txgi
"""

from IPython import get_ipython
from IPython.display import display

# Imports
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from flask import Flask, render_template, request, jsonify
from newsapi import NewsApiClient
import re
from sentence_transformers import SentenceTransformer
import tf_keras
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import os

# Initializing Variables - Important for rest of the program
app = Flask(__name__)
newsapi = NewsApiClient(api_key='4e6d0e08be6445ee927d5a5896090114')

# Configure Gemini API
# You'll need to set your Gemini API key as an environment variable or replace with your actual key
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', 'AIzaSyDVZw5gRtA75Ydr1RfDSu3tx1FRjM73CVg')
genai.configure(api_key=GEMINI_API_KEY)

# Initialize Gemini model
gemini_model = genai.GenerativeModel('gemini-1.5-flash')

# NewsAPI fetching articles
def fetch_articles(source, label, max_pages=5):
    all_articles = []
    for page in range(1, max_pages+1):
        resp = newsapi.get_everything(
            sources=source,
            language='en',
            sort_by='publishedAt',
            page_size=20,
            page=page
        )
        for art in resp.get('articles', []):
            all_articles.append({'text': art.get('content') or art.get('description'),
                                 'label': label,
                                 'source': source})
    return all_articles

# Example:
left_src = 'the-new-york-times,the-washington-post,msnbc'
right_src = 'fox-news,breitbart-news'
neutral_src = 'associated-press'  # mainstream center

left = fetch_articles(left_src, 'left')
right = fetch_articles(right_src, 'right')
neutral = fetch_articles(neutral_src, 'neutral')

df = pd.DataFrame(left + right + neutral).dropna(subset=['text'])

def clean_text(txt):
    if not isinstance(txt, str):
        return ""

    txt = txt.lower()  # normalize case
    txt = re.sub(r'\[.*?\]', '', txt)  # remove [bracketed] text
    txt = re.sub(r'https?://\S+', '', txt)  # remove links
    txt = re.sub(r'\s+', ' ', txt)  # collapse multiple whitespace into one space
    txt = txt.strip()  # remove leading/trailing whitespace

    # Optional: remove common footer junk
    footer_keywords = ["click here", "subscribe", "sign up", "advertisement"]
    for keyword in footer_keywords:
        txt = txt.replace(keyword, '')

    return txt

df['text'] = df['text'].apply(clean_text)

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)

print("Final data size:", len(df))
print("Label distribution:\n", df['label'].value_counts())

# Step 3: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    embeddings, df['label'], test_size=0.2, stratify=df['label'], random_state=42
)

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

# Testing
def predict_article_bias(article_text):
    # Clean (if you used cleaning before)
    cleaned = clean_text(article_text)

    # Embed
    embedding = model.encode([cleaned])[0]  # returns a list, so take [0]

    # Predict
    bias_prediction = clf.predict([embedding])[0]

    # Get prediction confidence/probabilities
    probabilities = clf.predict_proba([embedding])[0]
    confidence_scores = dict(zip(clf.classes_, probabilities))

    return bias_prediction, confidence_scores

def generate_gemini_analysis(article_text, bias_prediction, confidence_scores, article_url=None):
    """
    Use Gemini API to generate a detailed analysis of the news article bias
    """

    # Truncate article text if it's too long for the API
    article_excerpt = article_text[:2000] + "..." if len(article_text) > 2000 else article_text

    prompt = f"""
    You are an expert media analyst. I have analyzed a news article using machine learning and determined its political bias. Please provide a comprehensive analysis based on the following information:

    ARTICLE EXCERPT:
    {article_excerpt}

    BIAS PREDICTION: {bias_prediction}

    CONFIDENCE SCORES:
    {', '.join([f"{label}: {score:.2%}" for label, score in confidence_scores.items()])}

    URL: {article_url if article_url else "Not provided"}

    Please provide:
    1. A summary of the article's main points
    2. An explanation of why this article might be classified as "{bias_prediction}"
    3. Specific language, framing, or perspective indicators that suggest this bias
    4. The reliability of this prediction based on the confidence scores
    5. Recommendations for readers on how to approach this article critically

    Keep your response informative, objective, and educational. Focus on media literacy and helping readers understand bias detection.
    """

    try:
        response = gemini_model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error generating Gemini analysis: {str(e)}"

def get_article_from_url(url):
    """
    Extract article text from a given URL
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # Try to find article content in common HTML tags
        article_selectors = [
            'article',
            '[class*="article"]',
            '[class*="content"]',
            '[class*="story"]',
            'main'
        ]

        article_text = ""
        for selector in article_selectors:
            elements = soup.select(selector)
            if elements:
                paragraphs = elements[0].find_all("p")
                if paragraphs:
                    article_text = " ".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
                    break

        # Fallback to all paragraphs if no article container found
        if not article_text:
            paragraphs = soup.find_all("p")
            article_text = " ".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())

        return article_text
    except Exception as e:
        return f"Error extracting article: {str(e)}"

def analyze_article_with_gemini(article_text_or_url):
    """
    Complete analysis pipeline with Gemini integration
    """
    # Determine if input is URL or text
    if article_text_or_url.startswith(('http://', 'https://')):
        article_url = article_text_or_url
        article_text = get_article_from_url(article_url)
        if article_text.startswith("Error"):
            return {"error": article_text}
    else:
        article_text = article_text_or_url
        article_url = None

    # Get bias prediction
    bias_prediction, confidence_scores = predict_article_bias(article_text)

    # Generate Gemini analysis
    gemini_analysis = generate_gemini_analysis(article_text, bias_prediction, confidence_scores, article_url)

    return {
        "bias_prediction": bias_prediction,
        "confidence_scores": confidence_scores,
        "gemini_analysis": gemini_analysis,
        "article_url": article_url,
        "article_length": len(article_text)
    }

# Test the functionality
url = "https://www.theguardian.com/news/2025/jun/10/how-does-woke-start-winning-again?utm_source=chatgpt.com"
result = analyze_article_with_gemini(url)

print("=== BIAS PREDICTION ===")
print(f"Predicted bias: {result['bias_prediction']}")
print(f"Confidence scores: {result['confidence_scores']}")
print("\n=== GEMINI ANALYSIS ===")
print(result['gemini_analysis'])

# Server stuff with new endpoints
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_endpoint():
    """
    API endpoint for article analysis
    """
    data = request.get_json()

    if not data or 'input' not in data:
        return jsonify({"error": "No input provided"}), 400

    try:
        result = analyze_article_with_gemini(data['input'])
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/quick-predict', methods=['POST'])
def quick_predict():
    """
    Quick bias prediction without Gemini analysis
    """
    data = request.get_json()

    if not data or 'text' not in data:
        return jsonify({"error": "No text provided"}), 400

    try:
        bias_prediction, confidence_scores = predict_article_bias(data['text'])
        return jsonify({
            "bias_prediction": bias_prediction,
            "confidence_scores": confidence_scores
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    # run() method of Flask class runs the application
    # on the local development server.
    app.run(debug=True, port=8000)